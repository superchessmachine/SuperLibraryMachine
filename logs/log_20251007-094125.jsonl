{"timestamp": "2025-10-07T09:41:25.480026", "input_query": "Hi", "rewritten_query": "Could you please provide a detailed and specific question related to a particular aspect of structural biology, such as the molecular mechanisms of protein-protein interactions, high-resolution structural determination techniques (e.g., X-ray crystallography, cryo-electron microscopy), conformational dynamics of biomolecular complexes, or how specific mutations affect the three-dimensional structure and function of biological macromolecules?", "retrieved_chunks": ["[1] spread the risks inherent in the \ufb01nancial system and improve liquidity, as expressed by Shiller in [Shi08], as well as by many earlier research papers arguing that a more complete set of links between banks would lower the risk of system contagion [AB08]. This last concern was particularly motivated by the recent advances in network theory. Network thinking, that is, focusing on relationships between di\ufb00erent entities as the unit of study as well as the entities themselves, provided a novel language in describing many complex systems, particularly those where the complexity arises [DJW98] and Barab\u00b4asi and Albert [BA99] motivated a wide interest in networks and the search for unifying principles governing the networks found in di\ufb00erent branches to the spread of infectious diseases [Mit09]. Under the new direction an emphasis has been given to how the network anatomy a\ufb00ects the network dynamical behaviour and its stability, and paved the way to see similarities between networks in di\ufb00erent \ufb01elds of study. For example, Haldane\u2019s paper [Hal09] draws similarities between the SARS crisis, and argues that they all represent environments exposed to risk that can be studied under the dynamic network framework. The literature addressing banking networks can generally be classi\ufb01ed into two streams. The \ufb01rst, and more recent, is based on empirical studies that typically ceed to examine their stability. Within this stream, supported by multiple empirical tries), the current trend motivated by recent work in network theory is to start from so\u2013called scale\u2013free network of banks and impose a shock (endogenously or exoge- nously), observing the e\ufb00ect on the network in terms of observed losses. Such ap- proach emphasizes the interconnected nature of the network, in particular the notion of robust\u2013yet\u2013fragile systems, that is, robust with respect to random shocks a\ufb00ecting The second stream of literature, on the other hand, bears a more theoretical \ufb02avour. It starts with a simpli\ufb01ed model with typically a small number of homoge- neous banks and examines the interaction and stability of this small network under a Based on their nature, the two streams essentially targeted di\ufb00erent questions and as such on multiple occasions can be viewed as drawing di\ufb00erent conclusions. Amongst them, there are four major questions to be addressed for a full understanding of the", "[2] centrality, components, and cut sets play a key role in identifying the banks whose defaults yield a fragmented banking system. Another important measure is the clustering coe\ufb03cient, which is a measure of the network transitivity or \u2018local\u2019 connections density. It is de\ufb01ned as the probability that two vertices are connected to each other given that each of them is connected to a common third one (commonly corresponding to \u2018the friend of my friend is also my friend\u2019). In banking networks this is identi\ufb01ed as the probability that two banks have deposits with each other given that each of them has an interbank link with a third In general, this can be expressed as a triangle, or a loop of length three. As such the clustering coe\ufb03cient is de\ufb01ned as the total number of vertices forming triangles divided by the total number of triplets (i.e three connected vertices). clustering can be de\ufb01ned for single vertices. The clustering coe\ufb03cient for vertex i (and referred to as \u2018local clustering\u2019) is de\ufb01ned as number of pairs of neighbors of vertex i that are connected total number of pairs of neighbors of i The small\u2013world e\ufb00ect (also known as \u201csix degrees of separation\u201d), refers to the surprisingly short distance between vertices in a network. The feature is attributed to Milgram\u2019s experiment [AB09] of distributing letters to hundreds of random people aiming at reaching a particular person by sending the letter to another person they know on a \ufb01rst name basis. The question was how many steps it would take for the letter to reach the destination. The answer came to be only about six steps. Putting it mathematically, it refers to a small mean distance, i X between the vertices in a network. The wide use of the term arises from the profound implications of the phenomenon in networks coupled with the universality of its ex- istence when examining actual networks. The e\ufb00ect is crucially re\ufb02ected in banking networks as \u2018bad news for one bank being not too far o\ufb00 other banks\u2019. One of the most frequently used property of networks is the frequency distribution of the vertices. Let pk denote the fraction of vertices in the network having degree k. It follows that pk then represents the degree distribution of the network. For a directed network, as to be expected, there are two distributions, the in-degree distribution", "[3] on actual network structures as a starting point in examining the system stability, provided a rather static picture of the system as manifested by knocking one bank at a time and examining the domino e\ufb00ect, if any, on the network. Such an approach, while providing the \ufb02exibility of examining the network, neglected the dynamical nature of the system where banks as well as individual depositors a\ufb00ect each other, In addition, the approach lacked the motivating forces motives for banks to interact with each other. Rather the approach imposed a set of simplifying assumptions to compensate for the lack of such forces, such as the mechanical nature of bank failures by knocking them down and the zero recovery rates associated with their failure. On the other hand we \ufb01nd that theoretical studies such as that of Allen and Gale [AG98], by employing an economist\u2019s intuition in addressing bank failures and bank interaction, provided the interacting forces that the empirical based studies lacked. Nevertheless, theoretical studies building on assumptions of optimized behaviour as well as restricting network topologies hindered their generalization to more relevant settings or di\ufb00erent network structures. Between the static picture provided by the empirically based studies and the simpli\ufb01ed and constrained picture of the theoretical studies, the underlying theory of banking systems implicitly or explicitly re\ufb02ected an unrealistic picture of a manageable and predictable organization. We believe the more appropriate call for the banking theory is to combine the contributions of both streams, the \ufb02exibility of the empirical based studies and the quest to understand the underlying dynamical forces in theoretically based studies, in a new direction of Two core modi\ufb01cations to the assumptions of orthodox economic theory have re- cently been proposed, helping pave the way for such new directions of research. The \ufb01rst is an emphasis on inductive rather than deductive thinking. The deductive ap- proach for economic analysis has long been argued against by many economists given that our rationality is essentially bounded (the notion of bounded rationality goes back to Nobel prize laureate Herbert Simon in the 1950s [Sim55]). Recently, it has been explicitly argued that inductive reasoning (as put forward in the El Farol prob- lem by Arthur [Art94]) gives a proper candidate approach or model for our decision making. That approach implied a learning, adaptive process through which market participants form hypotheses given their set of information and experiences, test the", "[4] for tij is the shortest path length between nodes i and j in graph G. The measure is a modi\ufb01cation of using the shortest path length (diameter), with the advantages that it is well-de\ufb01ned for non-connected networks and is a global property of the network. Their results agree with those of Albert, Jeong and Barab\u00b4asi for the random On the other hand when considering the second and third criteria for choosing the important nodes to remove, they found that the results for the scale\u2013free network do not substantially di\ufb00er from these reported from choosing important nodes based on their degrees. The di\ufb00erence is more pronounced for the random graph case, where attacks based on a recalculated number of shortest paths cause a greater damage than that based on degree. They attributed these observations to the fact that in scale\u2013 free networks the nodes with the highest degree are also the nodes with the highest number of shortest paths, whereas for the random networks there is not a perfect The above discussion of error and attack tolerance should not be confused with an- other closely related yet di\ufb00erent topic of the cascading failures in complex networks. In our above discussion we examined the e\ufb00ect of node removal on the network con- nectivity, diameter or e\ufb00ectiveness, where it should be emphasized that upon the removal of the node, all its edges are also removed from the network. Then the re- sulting e\ufb00ect on the network is examined. The second class of models of cascading failures is addressing the e\ufb00ect of node removal or failure as the load on that failed node has to be rerouted to other nodes, which may eventually lead to an avalanche of overloads on other nodes that are not able to handle the extra load or tra\ufb03c. This class of models is sometimes referred to as the dynamic approach of node failures, opposed to the static one represented earlier. The main mechanism responsible for prehended in complex communication, transportation, electrical power grids systems and tra\ufb03c networks. Under such consideration, it has been shown that a single node failure can collapse the entire system due to the dynamics of the redistribution of its As we have just indicated, whereas the removal of the most connected links will a\ufb00ect the diameter of the network, the existence of a giant connected component does not depend on the presence of these highly connected nodes. In addition the giant", "[5] Our second main contribution is that we study different strategies that banks might use to deal with depositors\u2019 withdrawals and compare the consequences for systemic risk. To do this, we consider different rules that banks might use to reduce their lending to other banks when facing withdrawals. We characterise these rules in terms of a pecking order for each bank that determines the order in which it stops lending to other institutions. The mathematical setting for this is very general and includes many special cases of interest. For example, if all institutions have the same rank on bank i\u2019s pecking order, bank i would withdraw funding proportional from its borrowers. Another example would be that there is a strict pecking order according to some criterion, for example the size of the loans that bank i provided to other institutions. Bank i might choose to stop lending to its largest borrower first or it might stop lending to its smallest borrower first. Importantly, this pecking order allows us to also include additional information in the model that can determine the withdrawal process. So, while in our model banks withdraw funding because they are under stress and not because they have concern about their borrowers not being able to repay their debt, our model can include information about the borrowers\u2019 financial resilience. Hence, our modelling framework is flexible enough to include a wholesale freeze of the interbank market or operate in a more nuanced fashion which was important, for example, in the bank run episode in 1931 in Germany for which Blickle et al. (2024) showed that banks in the interbank market withdrew funding almost exclusively from failing banks and not from surviving banks. When analysing the spread of contagion via the interbank market it is important to recognise that, as already pointed out by Allen & Gale (2000), the interbank market reallocates liquidity but does not create liquidity. In particular, any interbank withdrawals ultimately have to be covered by either the liquid or the illiquid external assets. We show that even though interbank lending is only a small part of a bank\u2019s balance sheet, the strategies used to decide on reducing lending still significantly affect the spread and magnitude of contagion. Our third contribution is the introduction of several tractable measures to assess the con- sequences of initial bank runs. This allows us to analyse and compare different mitigating strategies that banks might use. These measures can also be used as a tool for financial reg- ulation, e.g., to assess and design liquidity buffers, deposit insurance, liquidity provision by a lender of last resort, etc.", "[6] networks) as to be expected with such topology. We \ufb01nd that while most banks in the scale\u2013free network will not trigger much damage to the system if defaulted (robustness under random failures of nodes), few banks will cause a great damage (fragile in face of targeted attacks). Our work supports the documented di\ufb00erences in the resilience of the two network topologies re\ufb02ecting the \u201crobust-yet-fragile\u201d nature of scale\u2013free The highly connected banks were the ones responsible for the greatest damage to the system in terms of capitalization loss and additional numbers of banks triggered to default, particularly in the case of scale\u2013free networks. The relationship between the defaulted banks connectivity is less apparent in the case of random networks. In terms of magnitude, the damage caused by the highly connected banks in the scale\u2013 free network reaches up to triple the maximum damaged witnessed in the random Moving further in our analysis and highlighting the role played by these \u201ctoo interconnected\u201d banks, we built 500 independent scale\u2013free networks each of 500 banks and measured the extent of damage caused by single\u2013bank failures. We separated our banks according to connectivity as one group consisting of the top 10 connected banks and another with the remaining banks. The connectivity of such highly connected banks reaches up to 23 times that of the rest of the other banks on average (in terms of in\u2013degree). We documented how on average the damage caused by one of the top connected banks measured by capitalization loss reaches up to 20 times that of other banks, and 25 times higher for the number of subsequently triggered defaults. We concluded this part of the thesis, by brie\ufb02y examining the e\ufb00ect of systemic events on our networks following the work of Cont and Santos [CS09]. We subjected our network to exogenously correlated shocks a\ufb00ecting all banks through a Monte Carlo simulation, reaching similar conclusions to theirs in that few highly connected banks cause the most systemic damage to our network. While the conclusion derived from such empirical and simulation\u2013based studies help to shape our understanding of the nature of the banking system, they are es- The essence of such empirically based studies is a snapshot view on the nature of the network, which is then subjected to a mechanical chain reaction to the initial bank knockout default. In such studies banks do not learn, anticipate, or react to other There is no doubt that additional features can be incorporated to such studies", "[7] them, there are four major questions to be addressed for a full understanding of the The \ufb01rst question of how the network came about and what motivated banks to interact in an interbank network, we believe to be better addressed in the theoretical stream set of studies, while the second question concerning the current shape and structure of the network is better addressed in the empirically based studies. The third question of the e\ufb00ect of network topology on its stability has been addressed by both streams of literature, nevertheless leading to what might be viewed as con- tradictory conclusions. The recent stream of empirical-based studies documented the network topology as being scale\u2013free and accordingly concluded the robust\u2013yet\u2013fragile nature of the network (see for example [CS09] on the Brazilian interbank market). On the other hand, theoretically based studies addressing network and the e\ufb00ect of topology on its stability told a di\ufb00erent story where the more connected the inter- bank network is the more stable it is. For example, the well-cited paper of Allen and Gale [AG98] supported the idea that connected complete networks strengthened the \ufb01nancial systems by spreading any losses that might be su\ufb00ered from a single bank throughout the network, thereby lowering the possibility of any systemic risk. The fourth question, regarding the evolution of the network, is rarely addressed, apart from a few empirical studies describing how the network characteristics change using Di\ufb00erences in results can be attributed to di\ufb00erent mechanisms for propagation of shocks in the banking network. The Allen and Gale model (which will be denoted by AG) considered shocks as an excess liquidity demand faced by single banks based on the Diamond and Dybvig model [DD00] (denoted by DD) and used the mechanism of direct interbank linkages as the source of contagion in the \ufb01nancial system. Similar models are found in Temzelides [Tem97], Dasgupta [Das04], and Romero [Rom09], except that the propagation of the shocks is mainly carried out by agents in the system rather than by direct transfer through interbank links. For example, [Rom09] considers a social network of agents, where the agent decides on early withdrawing by asking three of his eight neighbours if they have already withdrawn. On another strand we \ufb01nd papers like Cifuentes, Ferrucci and Shin [CFS05] and", "[8] growth (vertices are added gradually) and preferential linking (vertices are chosen proportional to their degree to be connected with the new vertex). In the Barab\u00b4asi and Albert model vertices are added one by one and each vertex connects to a randomly chosen vertex proportional to its degree. The number of connections for each vertex is \ufb01xed to be equal the average degree . A number of modi\ufb01cations have been i h proposed to their basic model, such as the possibility of not only adding edges but also removing edges, addition of extra edges, introducing a non-linear preferential Since the introduction of the Barab\u00b4asi-Albert model, a vast amount of work has been carried out to study scale\u2013free networks, either by studying more general mechanisms to construct such networks or by studying their properties, particularly those related to their stability in the face of shocks. Before going on to study the interbank market represented as scale-free networks, we aim \ufb01rst to shed more light into their nature As addressed by Barab\u00b4asi and Albert [BA99], the origin of the observed power\u2013law degree distribution in real networks has been attributed to two mechanisms: growth and preferential attachment. Their algorithm proceeds as follows: nodes in such a way that the probability \u03c0 that a new node is connected to an existing node i with degree ki is ki j kj In terms of banking networks, this corresponds to starting with a few initial banks and at each time step add one bank to the system connected to these previously establish banks with probability based on how many interbank links these old banks already have. and mt edges. It can be shown that it produces a network with a power\u2013law degree distribution of exponent 3. In addition, the authors showed in a subsequent paper are \u2018simultaneously\u2019 responsible for the scale-free nature of the network, and not one Before moving to more generalized models for generating scale-free networks, it is worth stating the properties of the networks produced by the Barab\u00b4asi and Albert Average path length: they showed that the average path length increases ap- proximately logarithmically with the number of nodes n, and is always smaller grees are uncorrelated, their model produces networks where correlations be- tween the degree of connected nodes develop spontaneously. Clustering Coe\ufb03cient: the clustering coe\ufb03cient of their scale\u2013free network is \ufb01ve times higher than that found in an Erd\u02ddos\u2013R\u00b4enyi random graph. Although", "[9] Why were these assets funded in wholesale markets as opposed to retail mar- kets? The sophistication of these assets required that creditors be highly informed to evaluate payo\ufb00s, especially given the absence of deposit insurance. The complicated asset payo\ufb00 structure also suggests that having a close working relationship with borrowers is advantageous. It served to reduce the possibility of any kind of \ufb01nancial malfeasance. Given these considerations, it makes sense that wholesale banks obtain funding in inter-bank markets. In these markets lenders are sophisticated \ufb01nancial institutions as opposed to relatively unsophisticated households in the retail market. Figure5 shows that much of the growth in leverage in wholesale banking involved short term borrowing. The \ufb01gure plots the levels of asset backed commercial pa- The graph shows the logarithm of the real value outstanding. Nominal values from Flow of FUnds are de\ufb02ated using the CPI per (ABCP) and repurchase agreements (Repo). This growth re\ufb02ected partly the growth in assets held by wholesale banks and partly innovation in loan securitiza- tion that made maturity transformation by wholesale banks more e\ufb03cient. Also relevant, however, was a shift in retail investors demand from longer term security tranches towards short term credit instruments as the initial fall in housing prices in 2006 raised concerns about the quality of existing securitized assets.11,12 As we discuss next, the combination of high leverage and short term debt is what made the wholesale banking system extremely fragile. 11See Brunnermeier and Oemke (2013) for a model in which investors prefer shorter maturities when realease of information could lead them not to roll over debt. 12It is not easy to gather direct evidence on this from the aggregate composition of liabilities of wholesale banks since data from the Flow of Funds excludes the balance sheets of SIVs and CDOs from the ABS Issuers category. Our narrative is based on indirect evidence coming from ABX spreads as documented for example in Gorton (2009). We emphasize that a distinctive feature of these two signi\ufb01cant waves of \ufb01nancial distress is that they did not involve traditional banking institutions. In fact, the retail sector as a whole was shielded thanks to prompt government intervention that halted the run on MMMFs in 2008 as well as the Troubled Asset Relief Program and other subsequent measures that supplemented the traditional safety net. In fact, total short term liabilities of the retail sector were little a\ufb00ected overall (See Figure 19). This allowed the retail banking sector to help absorb some of the intermediation", "[10] few banks will trigger multiple defaults (as high as causing another 4 banks to default). These high risk banks, as shown in Figure 4.7, have high in\u2013degree, nevertheless they are not necessarily the most interconnected banks. The relation between \u201ctoo interconnected\u201d and the system fragility is more appar- ent in terms of default impact DI. As we see in Figure 4.8, there is a clear positive relation between the two variables, with the most interconnected bank causing the t c a p m Figure 4.8: Relation between in\u2013degrees of defaulted banks and the percentage of capitalization lost in a scale\u2013free network. As mentioned earlier, the considerably small network in the example of the Brazilian banking system does not allow us to draw \ufb01rm conclusions in terms of the impact of network structure on its stability. As such, by enlarging our network to 500 nodes (producing 4,279 links in the scale\u2013free network), and keeping our parameter values di\ufb00erentiation between the e\ufb00ect of structure can be drawn. We also built a random network G(n,m) with n = 500 nodes and edges m = 4,279 and populate both networks using a Pareto distribution for exposures. For the scale\u2013free network we \ufb01nd banks with in\u2013degrees as high as almost 165 (hubs) compared to less than 20 in\u2013degree in the random network case, as shown in the histogram in Figure 6.4. In the same histograms, we see the clear di\ufb00erence in Figure 4.9: Histogram of banks in\u2013degree for scale\u2013free and random networks. For the histogram of default impact, it is obvious that the distribution of total losses in the two networks are di\ufb00erent in shape, with a sharper decline in the case of a scale\u2013free network. Nevertheless we \ufb01nd that few nodes will cause damage as much case a damage higher than 3.5%, as seen in Figure 4.10. In terms of the number of banks that have been triggered to default as a function of the in\u2013degree of the initially defaulted bank, in the scale\u2013free case we \ufb01nd that the number of defaulted banks is as high as 12, more than triple the number in the case of a random network. The e\ufb00ect of the highly connected banks is obvious from both", "[11] of a random network. The e\ufb00ect of the highly connected banks is obvious from both Highly connected banks cause the highest damage if compared to the less connected n o p a c n o i t a z a t i p a c have on its stability (in terms of percentage of capitalization lost and further number of banks triggered to default), our main aim is examining the stability of our network under the scale\u2013free network model. The following section is devoted to a closer From the simulation above, we observe that the most connected banks cause the most damage to the network under a single\u2013bank default scenario. In a further examination to that observation we carried 500 independent simulations (using the same parame- ters as above) for a scale\u2013free network of 500 banks. To examine the e\ufb00ect of these highly connected banks, we divided the banks in each of our networks into two sets: For each set, and for each simulated run of the network, we calculated the average in\u2013degree, average default impact and average number of banks triggered to default. The results are shown in Figure 4.13. t c a p m f o e g a r e v A e e r g e d n I e g a r e v A s k n a B d e t l u a f e D e g a r e v A Figure 4.13: Results for 500 Simulated Networks. Top 10 connected banks compared to rest of the banks in a scale\u2013free network of 500. As illustrated, there is a clear distinction between results for the two sets of banks. The \ufb01rst panel of Figure 4.13 indicates that on average a bank in the top 10 most connected will cause an average loss of 7.75% of capitalization for the system (corre- sponding to the average of the 500 simulated runs, with standard deviation of 0.02) compared to an average damage of 0.35% caused by a bank that is not in the top 10 (with standard deviation of 0.001), that is a top bank can cause a damage of 20 times more than that of a not highly connected bank. The second panel demonstrates the existence of hubs: for the \ufb01rst set the average is 143.8 in\u2013degrees compared to 6.16 the second). Finally the bottom graph of the \ufb01gure shows that the initial shock of", "[12] other words, \ufb01x a number n of vertices, where each pair has the same probability of being connected by an edge. As such we are sampling every edge of the total number In such n-vertex Erd\u02ddos\u2013R\u00b4enyi networks, a vertex is connected to the rest of the be connected to a particular set of k vertices, and not to the remaining (n vertices, is pk(1 p)n\u22121\u2212k, and there are of such k vertices. The probability of being connected to exactly k vertices is then which is a binomial distribution. As n a Poisson distribution, so the model is also known as the Poisson random graph. As the probability of any two vertices to be connected to a third one is \ufb01xed at p, it follows that the clustering coe\ufb03cient is C = p. Since it can be shown that p = hki n\u22121 denotes the mean degree, we have that i \u2212 While this model is one of the best studied and originally adopted for its simplicity, it has a number of problems that make it unrealistic to be used for real world appli- cations. First as it can be seen that the clustering coe\ufb03cient given above approaches 0 as the number of vertices approaches in\ufb01nity. Even for a \ufb01nite number of vertices the clustering coe\ufb03cient of random graphs is sharply smaller than the actual high clustering coe\ufb03cient observed in real world networks. Second, and most importantly, the shape of the degree distribution does not correspond to that observed in empirical networks, which are typically right skewed with a few number of vertices being highly connected \u201chubs\u201d, while most vertices have low degree. This was introduced by Watts and Strogatz in 1998 [Wat02b] with the motivation of understanding the origin of the small\u2013world e\ufb00ect. They believed it depends on two contradicting forces: a large clustering coe\ufb03cient (two friends of a common friend are more likely to know each other than two randomly chosen people), and the possibility to connect rather distant people to each other in only a few intermediates. Their goal was to tune a network between complete disorder (random graph), and complete The solution was a model starting with vertices arranged on a one dimensional line bent around like a circle, each connected to z vertices nearest to it and thus achieving a", "[13] not depend on the presence of these highly connected nodes. In addition the giant connected component will still display the small\u2013world feature after the removal of the highly connected nodes [ML02]. A more drastic e\ufb00ect is to be witnessed when the dynamic properties in terms of the \ufb02ow of the network is examined, as opposed to the static properties presented so far. In the paper by Motter and Lai [ML02] focusing on the cascades triggered by the removal of single node, they proposed a model where at each time step a single unit of information or \ufb02ow is exchanged between every pair of nodes, and this information takes the shortest path between the two nodes. De\ufb01ne the load Li of a node i as the total number of shortest paths going through it and the capacity of a node as the maximum amount of load the node can handle, which they proposed it to be a linear 0 is a tolerance parameter. Starting with a network of N-nodes, the removal of a node would trigger a change in the distribution of shortest paths, and accordingly of the load distribution, which might cause the failure of further nodes if their load exceeded their capacity. The process is called a cascade failure. The damage caused by such cascade is measured in terms of the change in largest connected and N are the number of nodes in the largest component after and before They then examined their model on the two types of networks, random and scale the highly loaded nodes. Under the scale\u2013free network model they found that, for a random removal of nodes, on average G remains close to unity. Yet under intentional attacks, there is signi\ufb01cant reduction in G, where for a high value \u03b1 = 1, the initial attack can reduce the size of G by more than 20%. The damage is larger for smaller values of \u03b1 (for \u03b1 = 0.2 the size of G is reduced by more than 60%). The results are not as dramatic for the random graph model, where the system does not experience any cascade failure for \u03b1 as small as 0.05, under either node nodes for the scale\u2013free network will reduce G to less than 10% of its original size. They concluded that the homogeneity of the random graphs makes them resilient to cascades. A remark to be noted on their model is that it follows from the way they assign loads that on average the nodes with higher links or edges are also those with", "[14] matrix would then be de\ufb01ned as 1 if there is an edge from node j to node i Unlike the undirected case, this matrix would generally be asymmetric. Edges can be carrying weights or strength (value to them), such as the size of the deposits in the case of banking networks, and we call the graph/network a weighted network in this case. Similarly, vertices can also have weights attached to them. While the weights can be presented directly in the adjacency matrix A, we prefer for our analysis purpose to introduce a new matrix denoted by L to carry the weights The degree of a node is the number of edges attached to it. In banking networks, the degree of a node (or a bank) is then the number of interbank links that the bank has. Let us denote the degree of node i by ki. Using the adjacency matrix, in an undirected network the degree can then be de\ufb01ned as: Given that we have m edges, and each having two ends attached to di\ufb00erent nodes, it follows that of a vertex in an undirected network is given by which can be written as c = 2m n Another measure related to the mean degree in a simple network is the density \u03c1, which is de\ufb01ned as the ratio between the number of edges present in a network to the The network is said to be dense if the ratio \u03c1 tends to a constant as n whereas if that ratio tends to 0 the network is said to be sparse. For the case of directed network, we have two additional measures for degrees. The \ufb01rst is the in-degree kin i , de\ufb01ned as the number of incoming edges to vertex i. In banking networks, this is the number of interbank links corresponding to deposits that other banks have made with bank i. The second is the out-degree kout , de\ufb01ned as the number of outgoing edges from vertex i. In banking networks, this is the number of deposits that bank i has made in other banks. It then follows that kin i = i=1 X The total number of degrees ki (total number of links of a vertex) would be the sum of the vertex in-degree and out-degree. The total number of edges m is then the total number of incoming edges (or out-going edges): kin i = Similarly we have the mean in-degree and mean out-degree kin i = A path is the sequence of pairs of vertices connected by an edge, the length of a path", "[15] to a homogeneous network where each node has approximately the same number of links. The second type are scale-free networks, where the connectivity distribution is described by a power law distribution. Unlike the previous case, it is an inhomoge- di\ufb00erent from zero. In comparing their robustness, Albert, Jeong and Barab\u00b4asi con- structed two networks with an equal number of nodes: a random network, that is, a Erd\u00a8os-R\u00b4enyi random graph (n,p), and a scale\u2013free graph. They then examined the e\ufb00ect of two types of node removal: a random node removal, where the node to be removed is chosen at random (an error or failure of a node), and a removal of nodes chosen according to their degree, such as removing the most connected nodes (an attack). The e\ufb00ect on the network was then examined using its diameter and giant Recall that the diameter is de\ufb01ned as the average length of the shortest paths connecting any two nodes and re\ufb02ects the nodes\u2019 ability to communicate with one another. The e\ufb00ect of removal of nodes on the network diameter can be summarized as follows: Error tolerance (random removal of nodes): when a small fraction f of nodes is removed there is in general an observed increase in the diameter for both types of networks (random and scale free). For the random network the diameter increases monotonically with f, which can be easily explained in terms of the homogeneity of the network nodes: since all nodes have approximately the same number of edges the removal of each node causes the same amount of increase in the diameter. On the other hand we \ufb01nd a drastically di\ufb00erent behaviour for the scale\u2013free network: the diameter remained unchanged above a certain fraction f (say 5% of node removal). This robustness to node failure is again attributed of nodes have only a small number of edges. Thus nodes with small connectivity will be the one most likely selected under the random removal scenario. That in turn would not alter the network diameter all that much. Attack survivability (removal of most connected nodes): an \u2018informed\u2019 network attacker will target the most connected nodes of the network. To simulate such an attack the authors removed nodes in a decreasing order in terms of their degree starting with the most connected one. Since in the random graph the nodes have approximately the same number of connections, there is no"], "final_answer": "Specific research question (structural-biology focus, inspired by the \u201chub-driven, robust-yet-fragile\u201d architecture seen in scale-free networks):\n\n\u201cIn a eukaryotic protein-interaction network that exhibits a scale-free topology in which a small number of \u2018hub\u2019 proteins dominate connectivity [6, 7], how does a disease-associated point mutation in the middle-domain \u2018hub interface\u2019 of the molecular chaperone Hsp90 (e.g., R380C) alter (i) the atomic-level conformational cycle of Hsp90 as determined by high-resolution single-particle cryo-electron microscopy (\u22643 \u00c5), (ii) the binding kinetics and geometry of its co-chaperone Aha1 and representative client kinases, and (iii) the propagation of structural perturbations through the wider interactome that make the system simultaneously robust to random partner loss yet fragile to targeted disruption of the hub [6]?\u201d", "citations": {"6": "not found", "7": "not found"}}
